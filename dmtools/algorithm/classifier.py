from dmtools.algorithm import *


class BaseModel(object):

    def __init__(self, **options):
        self.clf = []
        self.x_train = []
        self.y_train = []

        try:
            self.save_path = options["save_path"]
        except KeyError:
            self.save_path = None

        try:
            self.scoring = options["scoring"]
        except KeyError:
            self.scoring = None

        try:
            self.cv = options["cv"]
        except KeyError:
            self.cv = 5

    @abstractmethod
    def cross_val_score(self):
        return cross_val_score(self.clf, self.x_train, self.y_train, scoring=self.scoring, cv=self.cv)

    @abstractmethod
    def train(self):
        self.clf.fit(self.x_train, self.y_train)

    @abstractmethod
    def predict(self, y_test):
        return self.clf.predict(y_test)


class Svm(BaseModel):
    """
    æ”¯æŒå‘é‡æœº

    Cï¼š C-SVCçš„æƒ©ç½šå‚æ•°C,é»˜è®¤å€¼æ˜¯1.0. Cè¶Šå¤§ï¼Œç›¸å½“äºæƒ©ç½šæ¾å¼›å˜é‡ï¼Œå¸Œæœ›æ¾å¼›å˜é‡æ¥è¿‘0ï¼Œå³å¯¹è¯¯åˆ†ç±»çš„æƒ©ç½šå¢å¤§ï¼Œ
        è¶‹å‘äºå¯¹è®­ç»ƒé›†å…¨åˆ†å¯¹çš„æƒ…å†µï¼Œè¿™æ ·å¯¹è®­ç»ƒé›†æµ‹è¯•æ—¶å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†æ³›åŒ–èƒ½åŠ›å¼±ã€‚Cå€¼å°ï¼Œå¯¹è¯¯åˆ†ç±»çš„æƒ©ç½šå‡å°ï¼Œ
        å…è®¸å®¹é”™ï¼Œå°†ä»–ä»¬å½“æˆå™ªå£°ç‚¹ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå¼ºã€‚

    kernel ï¼šæ ¸å‡½æ•°ï¼Œé»˜è®¤æ˜¯rbfï¼Œå¯ä»¥æ˜¯â€˜linearâ€™, â€˜polyâ€™, â€˜rbfâ€™, â€˜sigmoidâ€™, â€˜precomputedâ€™
    ã€€ã€€0 â€“ çº¿æ€§ï¼šuâ€™v
    ã€€ã€€1 â€“ å¤šé¡¹å¼ï¼š(gamma*uâ€™*v + coef0)^degree
    ã€€ã€€2 â€“ RBFå‡½æ•°ï¼šexp(-gamma|u-v|^2)
    ã€€ã€€3 â€“sigmoidï¼štanh(gamma*uâ€™*v + coef0)

    degree ï¼šå¤šé¡¹å¼polyå‡½æ•°çš„ç»´åº¦ï¼Œé»˜è®¤æ˜¯3ï¼Œé€‰æ‹©å…¶ä»–æ ¸å‡½æ•°æ—¶ä¼šè¢«å¿½ç•¥ã€‚
    gamma ï¼š â€˜rbfâ€™,â€˜polyâ€™ å’Œâ€˜sigmoidâ€™çš„æ ¸å‡½æ•°å‚æ•°ã€‚é»˜è®¤æ˜¯â€™autoâ€™ï¼Œåˆ™ä¼šé€‰æ‹©1/n_features
    coef0 ï¼šæ ¸å‡½æ•°çš„å¸¸æ•°é¡¹ã€‚å¯¹äºâ€˜polyâ€™å’Œ â€˜sigmoidâ€™æœ‰ç”¨ã€‚
    probability ï¼šæ˜¯å¦é‡‡ç”¨æ¦‚ç‡ä¼°è®¡ï¼Ÿ.é»˜è®¤ä¸ºFalse
    shrinking ï¼šæ˜¯å¦é‡‡ç”¨shrinking heuristicæ–¹æ³•ï¼Œé»˜è®¤ä¸ºtrue
    tol ï¼šåœæ­¢è®­ç»ƒçš„è¯¯å·®å€¼å¤§å°ï¼Œé»˜è®¤ä¸º1e-3
    cache_size ï¼šæ ¸å‡½æ•°cacheç¼“å­˜å¤§å°ï¼Œé»˜è®¤ä¸º200
    class_weight ï¼šç±»åˆ«çš„æƒé‡ï¼Œå­—å…¸å½¢å¼ä¼ é€’ã€‚è®¾ç½®ç¬¬å‡ ç±»çš„å‚æ•°Cä¸ºweight*C(C-SVCä¸­çš„C)
    verbose ï¼šå…è®¸å†—ä½™è¾“å‡º
    max_iter ï¼šæœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚-1ä¸ºæ— é™åˆ¶ã€‚
    decision_function_shape ï¼šâ€˜ovoâ€™, â€˜ovrâ€™ or None, default=None3
    random_state ï¼šæ•°æ®æ´—ç‰Œæ—¶çš„ç§å­å€¼ï¼Œintå€¼
    """
    def __init__(self, x_train, y_train, **options):
        super(Svm, self).__init__(**options)
        self.x_train = x_train
        self.y_train = y_train

        try:
            self.param_C = options['C']
        except KeyError:
            self.param_C = 1

        try:
            self.kernel = options['kernel']
        except KeyError:
            self.kernel = 'rbf'

        try:
            self.max_iter = options['max_iter']
        except KeyError:
            self.max_iter = -1

        self.clf = svm.SVC(C=self.param_C, kernel=self.kernel, max_iter=self.max_iter)


class Knn(BaseModel):
    """
    æœ€é‚»è¿‘

    n_neighbors ï¼šé‚»è¿‘ç‚¹ä¸ªæ•°ï¼Œé»˜è®¤5ã€‚
    leaf_sizeï¼šä¸€èˆ¬é»˜è®¤æ˜¯30ã€‚å¯ä»¥åœ¨å…¶å€¼ä¸å¤§çš„èŒƒå›´å†…è°ƒè¯•çœ‹çœ‹æ•ˆæœã€‚
    weights ï¼šå‚æ•°æœ‰â€˜uniformâ€™å’Œâ€˜distanceâ€™ï¼Œå¯ä»¥é€‰æ‹©è°ƒè¯•ã€‚
    """

    def __init__(self, x_train, y_train, **options):
        super(Knn, self).__init__(**options)
        self.x_train = x_train
        self.y_train = y_train

        try:
            self.n_neighbors = options['n_neighbors']
        except KeyError:
            self.n_neighbors = 5

        try:
            self.weights = options['weights']
        except KeyError:
            self.weights = 'uniform'

        self.clf = neighbors.KNeighborsClassifier(n_neighbors=self.n_neighbors, weights=self.weights)


class GNB(BaseModel):
    """
    é«˜æ–¯æœ´ç´ è´å¶æ–¯

    priors : array-like, shape (n_classes,)
        Prior probabilities of the classes. If specified the priors are not
        adjusted according to the data.

    var_smoothing : float, optional (default=1e-9)
        Portion of the largest variance of all features that is added to
        variances for calculation stability.
    """

    def __init__(self, x_train, y_train, **options):
        super(GNB, self).__init__(**options)
        self.x_train = x_train
        self.y_train = y_train

        try:
            self.priors = options['priors']
        except KeyError:
            self.priors = None

        try:
            self.var_smoothing = options['var_smoothing']
        except KeyError:
            self.var_smoothing = 1e-9

        self.clf = GaussianNB(priors=self.priors, var_smoothing=self.var_smoothing)


class RandomForest(BaseModel):
    """
    éšæœºæ£®æ—

    n_estimators=100 : å†³ç­–æ ‘çš„ä¸ªæ•°
    criterion="gini" : gini or entropy(default=â€giniâ€)æ˜¯è®¡ç®—å±æ€§çš„gini(åŸºå°¼ä¸çº¯åº¦)è¿˜æ˜¯entropy(ä¿¡æ¯å¢ç›Š)ï¼Œæ¥é€‰æ‹©æœ€åˆé€‚çš„èŠ‚ç‚¹ã€‚
    max_depth=None : è®¾ç½®æ ‘çš„æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤ä¸ºNone å®¹æ˜“è¿‡æ‹Ÿåˆ
    min_samples_split=2 : æ ¹æ®å±æ€§åˆ’åˆ†èŠ‚ç‚¹æ—¶ï¼Œæ¯ä¸ªåˆ’åˆ†æœ€å°‘çš„æ ·æœ¬æ•°
    min_samples_leaf=1 : å¶å­èŠ‚ç‚¹æœ€å°‘çš„æ ·æœ¬æ•°
    min_weight_fraction_leaf=0. : å¶å­èŠ‚ç‚¹æ‰€éœ€è¦çš„æœ€å°æƒå€¼
    max_features="auto" : é€‰æ‹©æœ€é€‚å±æ€§æ—¶åˆ’åˆ†çš„ç‰¹å¾ä¸èƒ½è¶…è¿‡æ­¤å€¼
    max_leaf_nodes=None :  æœ€å¤§å¶èŠ‚ç‚¹æ•°
    min_impurity_decrease=0. : ä¸çº¯æ€§çš„å‡å°‘å¤§äºæˆ–ç­‰äºæ­¤å€¼ï¼Œåˆ™æ‹†åˆ†
    min_impurity_split=None : ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸å­˜æ€§å¤§äºæ­¤é˜ˆå€¼åˆ™æ‹†åˆ†ï¼Œå¦åˆ™ä¸ºå¶èŠ‚ç‚¹
    bootstrap=True : æ˜¯å¦æœ‰æ”¾å›çš„é‡‡æ ·
    oob_score=False : åœ¨æŸæ¬¡å†³ç­–æ ‘è®­ç»ƒä¸­æ²¡æœ‰è¢«bootstrapé€‰ä¸­çš„æ•°æ®ã€‚å¤šå•ä¸ªæ¨¡å‹çš„å‚æ•°è®­ç»ƒï¼Œæˆ‘ä»¬çŸ¥é“å¯ä»¥ç”¨cross validationï¼ˆcvï¼‰æ¥è¿›è¡Œï¼Œ
                      ä½†æ˜¯ç‰¹åˆ«æ¶ˆè€—æ—¶é—´ï¼Œè€Œä¸”å¯¹äºéšæœºæ£®æ—è¿™ç§æƒ…å†µä¹Ÿæ²¡æœ‰å¤§çš„å¿…è¦ï¼Œæ‰€ä»¥å°±ç”¨è¿™ä¸ªæ•°æ®å¯¹å†³ç­–æ ‘æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œç®—æ˜¯ä¸€ä¸ªç®€å•çš„
                      äº¤å‰éªŒè¯ã€‚æ€§èƒ½æ¶ˆè€—å°ï¼Œä½†æ˜¯æ•ˆæœä¸é”™
    n_jobs=None : å¹¶è¡Œjobä¸ªæ•°,CPUæœ‰å¤šå°‘coreï¼Œå°±å¯ä»¥å¯åŠ¨å¤šå°‘job
    random_state=None :
    verbose=0 :
    warm_start=False : çƒ­å¯åŠ¨ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ä¸Šæ¬¡è°ƒç”¨è¯¥ç±»çš„ç»“æœç„¶åå¢åŠ æ–°çš„
    class_weight=None : å„ä¸ªlabelçš„æƒé‡
    ccp_alpha=0.0,
    max_samples=None : å¦‚æœbootstrapæ˜¯trueï¼Œæ¯ä¸€æ¬¡è¿­ä»£é‡‡æ ·æ•°
    """

    def __init__(self, x_train, y_train, **options):
        super(RandomForest, self).__init__(**options)
        self.x_train = x_train
        self.y_train = y_train

        try:
            self.n_estimators = options['n_estimators']
        except KeyError:
            self.n_estimators = 100

        try:
            self.max_depth = options['max_depth']
        except KeyError:
            self.max_depth = 6

        try:
            self.min_samples_split = options['min_samples_split']
        except KeyError:
            self.min_samples_split = 2

        try:
            self.min_samples_leaf = options['min_samples_leaf']
        except KeyError:
            self.min_samples_leaf = 1

        try:
            self.max_features = options['max_features']
        except KeyError:
            self.max_features = None

        try:
            self.random_state = options['random_state']
        except KeyError:
            self.random_state = None

        self.clf = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, min_samples_split=self.min_samples_split,
                                          min_samples_leaf=self.min_samples_leaf, max_features=self.max_features, random_state=self.random_state)


class Xgb(BaseModel):
    """
    Xgboost

    max_depth : int
        åŸºæœ¬å­¦ä¹ å™¨æ ‘æ·±.
    learning_rate : float
        å­¦ä¹ ç‡
    n_estimators : int
        Number of trees to fit.
    verbosity : int
        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).
    silent : boolean
        Whether to print messages while running boosting. Deprecated. Use verbosity instead.
    objective : string or callable
        Specify the learning task and the corresponding learning objective. The objective options are below:
        reg:squarederror: regression with squared loss.
        reg:squaredlogerror: regression with squared log loss 12[ğ‘™ğ‘œğ‘”(ğ‘ğ‘Ÿğ‘’ğ‘‘+1)âˆ’ğ‘™ğ‘œğ‘”(ğ‘™ğ‘ğ‘ğ‘’ğ‘™+1)]2
            . All input labels are required to be greater than -1. Also, see metric rmsle for possible issue with this objective.
        reg:logistic: logistic regression
        binary:logistic: logistic regression for binary classification, output probability
        binary:logitraw: logistic regression for binary classification, output score before logistic transformation
        binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.
        count:poisson â€“poisson regression for count data, output mean of poisson distribution
            max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization)
        survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR).
        multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)
        multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.
        rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
        rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
        rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized
        reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed.
        reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.
    booster: string
        Specify which booster to use: gbtree, gblinear or dart. default= gbtree
    n_jobs : int
        Number of parallel threads used to run xgboost.  (replaces ``nthread``)
    gamma : float
        Minimum loss reduction required to make a further partition on a leaf node of the tree.
    min_child_weight : int
        Minimum sum of instance weight(hessian) needed in a child.
    max_delta_step : int
        Maximum delta step we allow each tree's weight estimation to be.
    subsample : float
        Subsample ratio of the training instance.
    colsample_bytree : float
        Subsample ratio of columns when constructing each tree.
    colsample_bylevel : float
        Subsample ratio of columns for each level.
    colsample_bynode : float
        Subsample ratio of columns for each split.
    reg_alpha : float (xgb's alpha)
        L1 regularization term on weights
    reg_lambda : float (xgb's lambda)
        L2 regularization term on weights
    """

    def __init__(self, x_train, y_train, **options):
        super(Xgb, self).__init__(**options)
        self.x_train = x_train
        self.y_train = y_train

        try:
            self.n_estimators = options['n_estimators']
        except KeyError:
            self.n_estimators = 100

        try:
            self.max_depth = options['max_depth']
        except KeyError:
            self.max_depth = 6

        try:
            self.objective = options['objective']
        except KeyError:
            self.objective = "multi:softmax"

        try:
            self.random_state = options['random_state']
        except KeyError:
            self.random_state = 0

        self.clf = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, objective=self.objective, random_state=self.random_state)
